{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76c5db8b",
   "metadata": {},
   "source": [
    "# Q1: Write a python program to scrape data for ‚ÄúData Analyst‚Äù Job position in ‚ÄúBangalore‚Äù location. You\n",
    "#have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10\n",
    "jobs data.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.naukri.com/\n",
    "2. Enter ‚ÄúData Analyst‚Äù in ‚ÄúSkill, Designations, Companies‚Äù field and enter ‚ÄúBangalore‚Äù in ‚Äúenter the\n",
    "location‚Äù field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data.\n",
    "Note: All of the above steps have to be done in code. No step is to be done manually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "6a4cbea1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Location Of Job</th>\n",
       "      <th>Name Of Companies</th>\n",
       "      <th>Experience Required</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sr.Business Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru, karnataka\\n(WFH during Co...</td>\n",
       "      <td>Collabera</td>\n",
       "      <td>6-11 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Analyst - CRM Platform</td>\n",
       "      <td>Bangalore/Bengaluru, Mumbai, Hyderabad/Secunde...</td>\n",
       "      <td>Artech infosystem</td>\n",
       "      <td>1-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Clarivate</td>\n",
       "      <td>2-4 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hiring Data Analyst - Forensics, KPMG India</td>\n",
       "      <td>Bangalore/Bengaluru, Noida, Kolkata, Mumbai, G...</td>\n",
       "      <td>KPMG India</td>\n",
       "      <td>2-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Analyst/Senior Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Meesho</td>\n",
       "      <td>3-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Senior Data Analyst II</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Flipkart</td>\n",
       "      <td>3-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Business Analyst / Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru, Mumbai, Hyderabad/Secunde...</td>\n",
       "      <td>cliqhr.com</td>\n",
       "      <td>5-10 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Software Technical Expert - Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Schneider Electric</td>\n",
       "      <td>3-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Reference Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Deutsche Bank</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sales Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>NUANCE INDIA PVT LTD</td>\n",
       "      <td>1-3 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Job Title  \\\n",
       "0                     Sr.Business Data Analyst   \n",
       "1                  Data Analyst - CRM Platform   \n",
       "2                                 Data Analyst   \n",
       "3  Hiring Data Analyst - Forensics, KPMG India   \n",
       "4             Data Analyst/Senior Data Analyst   \n",
       "5                       Senior Data Analyst II   \n",
       "6              Business Analyst / Data Analyst   \n",
       "7     Software Technical Expert - Data Analyst   \n",
       "8                       Reference Data Analyst   \n",
       "9                           Sales Data Analyst   \n",
       "\n",
       "                                     Location Of Job     Name Of Companies  \\\n",
       "0  Bangalore/Bengaluru, karnataka\\n(WFH during Co...             Collabera   \n",
       "1  Bangalore/Bengaluru, Mumbai, Hyderabad/Secunde...     Artech infosystem   \n",
       "2                                Bangalore/Bengaluru             Clarivate   \n",
       "3  Bangalore/Bengaluru, Noida, Kolkata, Mumbai, G...            KPMG India   \n",
       "4                                Bangalore/Bengaluru                Meesho   \n",
       "5                                Bangalore/Bengaluru              Flipkart   \n",
       "6  Bangalore/Bengaluru, Mumbai, Hyderabad/Secunde...            cliqhr.com   \n",
       "7                                Bangalore/Bengaluru    Schneider Electric   \n",
       "8                                Bangalore/Bengaluru         Deutsche Bank   \n",
       "9                                Bangalore/Bengaluru  NUANCE INDIA PVT LTD   \n",
       "\n",
       "  Experience Required  \n",
       "0            6-11 Yrs  \n",
       "1             1-6 Yrs  \n",
       "2             2-4 Yrs  \n",
       "3             2-6 Yrs  \n",
       "4             3-6 Yrs  \n",
       "5             3-6 Yrs  \n",
       "6            5-10 Yrs  \n",
       "7             3-6 Yrs  \n",
       "8             2-5 Yrs  \n",
       "9             1-3 Yrs  "
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing all required liberaries\n",
    "\n",
    "import selenium\n",
    "import pandas as pd\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Importing selenium webdriver \n",
    "from selenium import webdriver\n",
    "\n",
    "# Importing required Exceptions which needs to handled\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "\n",
    "#Importing requests\n",
    "import requests\n",
    "\n",
    "# importing regex\n",
    "import re\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "\n",
    "url = 'https://www.naukri.com/'\n",
    "driver.get(url)\n",
    "\n",
    "search_job = driver.find_element(By.CLASS_NAME,\"suggestor-input \")\n",
    "search_job\n",
    "\n",
    "search_job.send_keys(\"Data Analyst\")\n",
    "\n",
    "search_location = driver.find_element(By.XPATH,\"/html/body/div[1]/div[2]/div[3]/div/div/div[5]/div/div/div/input\")\n",
    "search_location\n",
    "search_location.send_keys(\"Bangalore\")\n",
    "\n",
    "search_btn = driver.find_element(By.XPATH,\"/html/body/div[1]/div[2]/div[3]/div/div[1]/div[6]\")\n",
    "search_btn\n",
    "search_btn.click()\n",
    "job_title = driver.find_elements(By.XPATH, '//a[@class = \"title fw500 ellipsis\"]')\n",
    "\n",
    "len(job_title)\n",
    "job_titles = []\n",
    "\n",
    "for i in job_title:\n",
    " job_titles.append(i.text)\n",
    "len(job_titles)\n",
    "job_titles\n",
    "\n",
    "\n",
    "job_Location = driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi location\"]')\n",
    "len(job_Location)\n",
    "job_Locations = []\n",
    "for i in job_Location:\n",
    " job_Locations.append(i.text)\n",
    "job_Locations\n",
    "\n",
    "\n",
    "\n",
    "company_name= driver.find_elements(By.XPATH, '//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "\n",
    "len(company_name)\n",
    "company_names = []\n",
    "\n",
    "for i in company_name:\n",
    "    \n",
    " company_names.append(i.text)\n",
    "\n",
    "len(company_names)\n",
    "company_names\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "exp_req = driver.find_elements(By.XPATH, '//li[@class=\"fleft grey-text br2 placeHolderLi experience\"]')\n",
    "\n",
    "len(exp_req)\n",
    "experience = []\n",
    "for i in exp_req:\n",
    "    \n",
    "  experience.append(i.text)\n",
    "len(experience)\n",
    "experience\n",
    "\n",
    "\n",
    "jobs= pd.DataFrame()\n",
    "jobs['Job Title']= job_titles\n",
    "jobs['Location Of Job']= job_Locations\n",
    "jobs['Name Of Companies']=Company_names\n",
    "jobs['Experience Required']=experience\n",
    "jobs\n",
    "jobs.head(10)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "48fb9762",
   "metadata": {},
   "source": [
    "Q2: Write a python program to scrape data for ‚ÄúData Scientist‚Äù Job position in ‚ÄúBangalore‚Äù location. You\n",
    "have to scrape the job-title, job-location, company_name. You have to scrape first 10 jobs data.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.naukri.com/\n",
    "2. Enter ‚ÄúData Scientist‚Äù in ‚ÄúSkill, Designations, Companies‚Äù field and enter ‚ÄúBangalore‚Äù in ‚Äúenter the\n",
    "location‚Äù field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fec127f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Location Of Job</th>\n",
       "      <th>Name Of Companies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Urgent Job Opening For AI Practitioner - Data ...</td>\n",
       "      <td>Bangalore/Bengaluru, Kochi/Cochin, New Delhi, ...</td>\n",
       "      <td>Wipro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hiring For Senior Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru, Pune</td>\n",
       "      <td>TATA CONSULTANCY SERVICES (TCS)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dataiku Consultant</td>\n",
       "      <td>Bangalore/Bengaluru, Pune, Chennai</td>\n",
       "      <td>Wipro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Applied Materials</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data &amp; Analytics Tech - Informatica Cloud- Sen...</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>PwC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Scientist: Advanced Analytics</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>IBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Research Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>IBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Principal - Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Schneider Electric</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Research and Development -AI/ML -(PhD )</td>\n",
       "      <td>Bangalore/Bengaluru, Noida, Hyderabad/Secunder...</td>\n",
       "      <td>EXL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Opportunity For Data Scientist - Female Candid...</td>\n",
       "      <td>Bangalore/Bengaluru, Gurgaon/Gurugram, Mumbai ...</td>\n",
       "      <td>PayU</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Job Title  \\\n",
       "0  Urgent Job Opening For AI Practitioner - Data ...   \n",
       "1                   Hiring For Senior Data Scientist   \n",
       "2                                 Dataiku Consultant   \n",
       "3                                     Data Scientist   \n",
       "4  Data & Analytics Tech - Informatica Cloud- Sen...   \n",
       "5                 Data Scientist: Advanced Analytics   \n",
       "6                                 Research Scientist   \n",
       "7                         Principal - Data Scientist   \n",
       "8            Research and Development -AI/ML -(PhD )   \n",
       "9  Opportunity For Data Scientist - Female Candid...   \n",
       "\n",
       "                                     Location Of Job  \\\n",
       "0  Bangalore/Bengaluru, Kochi/Cochin, New Delhi, ...   \n",
       "1                          Bangalore/Bengaluru, Pune   \n",
       "2                 Bangalore/Bengaluru, Pune, Chennai   \n",
       "3                                Bangalore/Bengaluru   \n",
       "4                                Bangalore/Bengaluru   \n",
       "5                                Bangalore/Bengaluru   \n",
       "6                                Bangalore/Bengaluru   \n",
       "7                                Bangalore/Bengaluru   \n",
       "8  Bangalore/Bengaluru, Noida, Hyderabad/Secunder...   \n",
       "9  Bangalore/Bengaluru, Gurgaon/Gurugram, Mumbai ...   \n",
       "\n",
       "                 Name Of Companies  \n",
       "0                            Wipro  \n",
       "1  TATA CONSULTANCY SERVICES (TCS)  \n",
       "2                            Wipro  \n",
       "3                Applied Materials  \n",
       "4                              PwC  \n",
       "5                              IBM  \n",
       "6                              IBM  \n",
       "7               Schneider Electric  \n",
       "8                              EXL  \n",
       "9                             PayU  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import selenium \n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "options = Options()\n",
    "import pandas as pd\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "\n",
    "url = 'https://www.naukri.com/'\n",
    "driver.get(url)\n",
    "\n",
    "search_job = driver.find_element(By.CLASS_NAME,\"suggestor-input \")\n",
    "search_job\n",
    "\n",
    "search_job.send_keys(\"Data Scientist\")\n",
    "\n",
    "search_location = driver.find_element(By.XPATH,\"/html/body/div[1]/div[2]/div[3]/div/div/div[5]/div/div/div/input\")\n",
    "search_location\n",
    "search_location.send_keys(\"Bangalore\")\n",
    "\n",
    "search_btn = driver.find_element(By.XPATH,\"/html/body/div[1]/div[2]/div[3]/div/div[1]/div[6]\")\n",
    "search_btn\n",
    "search_btn.click()\n",
    "\n",
    "job_title = driver.find_elements(By.XPATH, '//a[@class = \"title fw500 ellipsis\"]')\n",
    "len(job_title)\n",
    "\n",
    "job_titles = []\n",
    "for i in job_title:\n",
    " job_titles.append(i.text)\n",
    "job_titles\n",
    "\n",
    "\n",
    "job_Location = driver.find_elements(By.XPATH,'//li[@class=\"fleft grey-text br2 placeHolderLi location\"]')\n",
    "len(job_Location)\n",
    "job_Locations = []\n",
    "for i in job_Location:\n",
    " job_Locations.append(i.text)\n",
    "job_Locations\n",
    "\n",
    "\n",
    "\n",
    "company_name = driver.find_elements(By.XPATH, '//a[@class= \"subTitle ellipsis fleft\"]')\n",
    "\n",
    "len(company_name)\n",
    "\n",
    "company_names = []\n",
    "\n",
    "for i in company_name:\n",
    "    \n",
    " company_names.append(i.text)\n",
    "company_names\n",
    "\n",
    "#len(company_names)\n",
    "\n",
    "\n",
    "\n",
    "jobs= pd.DataFrame()\n",
    "\n",
    "jobs['Job Title']= job_titles\n",
    "jobs['Location Of Job']= job_Locations\n",
    "jobs['Name Of Companies']=company_names\n",
    "\n",
    "jobs\n",
    "jobs.head(10)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "864522dd",
   "metadata": {},
   "source": [
    "Ques3:\n",
    "\n",
    "In this question you have to scrape data using the filters available on the webpage as shown below:\n",
    "ASSIGNMENT 2\n",
    "You have to use the location and salary filter.\n",
    "You have to scrape data for ‚ÄúData Scientist‚Äù designation for first 10 job results.\n",
    "You have to scrape the job-title, job-location, company name, experience required.\n",
    "The location filter to be used is ‚ÄúDelhi/NCR‚Äù. The salary filter to be used is ‚Äú3-6‚Äù lakhs\n",
    "The task will be done as shown in the below steps:\n",
    "1. first get the webpage https://www.naukri.com/\n",
    "2. Enter ‚ÄúData Scientist‚Äù in ‚ÄúSkill, Designations, and Companies‚Äù field.\n",
    "3. Then click the search button.\n",
    "4. Then apply the location filter and salary filter by checking the respective boxes\n",
    "5. Then scrape the data for the first 10 jobs results you get.\n",
    "6. Finally create a dataframe of the scraped data.\n",
    "Note: All of the above steps have to be done in code. No step is to be done manually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6ce4e4f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>experience_required</th>\n",
       "      <th>company_name</th>\n",
       "      <th>job_location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DigitalBCG GAMMA Data Scientist</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "      <td>Boston Consulting Group</td>\n",
       "      <td>New Delhi, Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Senior Associate - Data Science</td>\n",
       "      <td>4-7 Yrs</td>\n",
       "      <td>(113 Reviews)</td>\n",
       "      <td>Mumbai, Hyderabad/Secunderabad, Gurgaon/Gurugr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist - Noida/Bangalore</td>\n",
       "      <td>5-10 Yrs</td>\n",
       "      <td>Black Turtle</td>\n",
       "      <td>Noida, Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist For Healthcare Product team</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "      <td>(54 Reviews)</td>\n",
       "      <td>Delhi / NCR, Chennai, Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist For Healthcare Product team</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "      <td>EXL</td>\n",
       "      <td>Delhi / NCR, Chennai, Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Scientist - Machine learning AI</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "      <td>(928 Reviews)</td>\n",
       "      <td>Delhi / NCR, Bangalore/Bengaluru, Mumbai (All ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Scientist - MIND Infotech</td>\n",
       "      <td>4-8 Yrs</td>\n",
       "      <td>8KMiles Software Services</td>\n",
       "      <td>Noida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Scientist - Engine Algorithm</td>\n",
       "      <td>1-3 Yrs</td>\n",
       "      <td>(15 Reviews)</td>\n",
       "      <td>Delhi / NCR, Kolkata, Mumbai, Hyderabad/Secund...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Knowledge/Data Scientist</td>\n",
       "      <td>3-6 Yrs</td>\n",
       "      <td>8KMiles Software Services</td>\n",
       "      <td>Delhi / NCR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>2-4 Yrs</td>\n",
       "      <td>(15 Reviews)</td>\n",
       "      <td>Delhi / NCR, Pune, Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    job_title experience_required  \\\n",
       "0             DigitalBCG GAMMA Data Scientist             2-5 Yrs   \n",
       "1             Senior Associate - Data Science             4-7 Yrs   \n",
       "2            Data Scientist - Noida/Bangalore            5-10 Yrs   \n",
       "3  Data Scientist For Healthcare Product team             2-7 Yrs   \n",
       "4  Data Scientist For Healthcare Product team             2-7 Yrs   \n",
       "5        Data Scientist - Machine learning AI             3-8 Yrs   \n",
       "6              Data Scientist - MIND Infotech             4-8 Yrs   \n",
       "7           Data Scientist - Engine Algorithm             1-3 Yrs   \n",
       "8                    Knowledge/Data Scientist             3-6 Yrs   \n",
       "9                              Data Scientist             2-4 Yrs   \n",
       "\n",
       "                company_name  \\\n",
       "0    Boston Consulting Group   \n",
       "1              (113 Reviews)   \n",
       "2               Black Turtle   \n",
       "3               (54 Reviews)   \n",
       "4                        EXL   \n",
       "5              (928 Reviews)   \n",
       "6  8KMiles Software Services   \n",
       "7               (15 Reviews)   \n",
       "8  8KMiles Software Services   \n",
       "9               (15 Reviews)   \n",
       "\n",
       "                                        job_location  \n",
       "0                     New Delhi, Bangalore/Bengaluru  \n",
       "1  Mumbai, Hyderabad/Secunderabad, Gurgaon/Gurugr...  \n",
       "2                         Noida, Bangalore/Bengaluru  \n",
       "3          Delhi / NCR, Chennai, Bangalore/Bengaluru  \n",
       "4          Delhi / NCR, Chennai, Bangalore/Bengaluru  \n",
       "5  Delhi / NCR, Bangalore/Bengaluru, Mumbai (All ...  \n",
       "6                                              Noida  \n",
       "7  Delhi / NCR, Kolkata, Mumbai, Hyderabad/Secund...  \n",
       "8                                        Delhi / NCR  \n",
       "9             Delhi / NCR, Pune, Bangalore/Bengaluru  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\") \n",
    "time.sleep(4)\n",
    "\n",
    "url = \"https://www.naukri.com/\"\n",
    "driver.get(url)\n",
    "time.sleep(6)\n",
    "\n",
    "# entering ‚ÄúData Scientist‚Äù in ‚ÄúSkill,Designations,Companies‚Äù field\n",
    "\n",
    "search_field_designation=driver.find_element(By.CLASS_NAME,\"suggestor-input \")   #job  search bar\n",
    "search_field_designation.send_keys(\"Data Scientist\")\n",
    "\n",
    "# clicking the search button\n",
    "search_btn = driver.find_element(By.XPATH,\"/html/body/div[1]/div[2]/div[3]/div/div[1]/div[6]\")\n",
    "search_btn\n",
    "search_btn.click()\n",
    "\n",
    "\n",
    "# creating empty lists for scraping data\n",
    "job_title=[]\n",
    "job_location=[]\n",
    "company_name=[]\n",
    "experience_required=[]\n",
    "\n",
    "\n",
    "time.sleep(5)\n",
    "#finding the delhi/ncr check box\n",
    "loc=driver.find_element(By.XPATH,\"/html/body/div[1]/div[3]/div[2]/section[1]/div[2]/div[5]/div[2]/div[3]/label/i\")\n",
    "loc.click()\n",
    "time.sleep(5)\n",
    "\n",
    "\n",
    "# finding the salary check box\n",
    "slry_box = driver.find_element(By.XPATH,\"/html/body/div[1]/div[3]/div[2]/section[1]/div[2]/div[6]/div[2]/div[2]/label/i\")\n",
    "slry_box.click()\n",
    "time.sleep(5)\n",
    "\n",
    "#scraping the job-titles\n",
    "titles=driver.find_elements(By.XPATH,\"//a[@class='title fw500 ellipsis']\")\n",
    "for i in titles:\n",
    "    if i.text is None :\n",
    "        job_title.append(\"--\") \n",
    "    else:\n",
    "        job_title.append(i.text)\n",
    "        \n",
    "time.sleep(5)\n",
    "\n",
    "\n",
    "#scraping the job-location\n",
    "locations=driver.find_elements(By.XPATH,\"//li[@class='fleft grey-text br2 placeHolderLi location']/span[1]\")\n",
    "for i in locations:\n",
    "    if i.text is None :\n",
    "        job_location.append(\"--\") \n",
    "    else:\n",
    "        job_location.append(i.text)\n",
    "        \n",
    "time.sleep(5)\n",
    "#scraping the company_name \n",
    "\n",
    "\n",
    "companies=driver.find_elements(By.XPATH,\"//div[@class='mt-7 companyInfo subheading lh16']/a\")\n",
    "for i in companies:\n",
    "    if i.text is None :\n",
    "        company_name.append(\"--\") \n",
    "    else:\n",
    "        company_name.append(i.text)\n",
    "        \n",
    "time.sleep(5)\n",
    "\n",
    "\n",
    "#scraping the experience_required \n",
    "experience=driver.find_elements(By.XPATH,\"//li[@class='fleft grey-text br2 placeHolderLi experience']/span[1]\")\n",
    "for i in experience:\n",
    "    if i.text is None :\n",
    "            experience_required.append(\"--\") \n",
    "    else:\n",
    "            experience_required.append(i.text)\n",
    "time.sleep(5)    \n",
    "\n",
    " # creating the dataframe from the scraped data and taking only first 10 jobs\n",
    "df=pd.DataFrame({\"job_title\":job_title[0:10],\"experience_required\":experience_required[0:10],\"company_name\":company_name[0:10],\n",
    "                 \"job_location\":job_location[0:10]})\n",
    "\n",
    "df\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "63da2d54",
   "metadata": {},
   "source": [
    "Ques 4\n",
    "Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "The attributes which you have to scrape is ticked marked in the below image.\n",
    "To scrape\n",
    "To scrape the data you have to go through following steps:\n",
    "1. Go to Flipkart webpage by url : https://www.flipkart.com/\n",
    "2. Enter ‚Äúsunglasses‚Äù in the search field where ‚Äúsearch for products, brands and more‚Äù is written and\n",
    "click the search icon\n",
    "3. After that you will reach to the page having a lot of sunglasses. From this page you can scrap the\n",
    "required data as usual.\n",
    "4. After scraping data from the first page, go to the ‚ÄúNext‚Äù Button at the bottom other page , then\n",
    "click on it.\n",
    "5. Now scrape data from this page as usual\n",
    "6. Repeat this until you get data for 100 sunglasses.\n",
    "Note: That all of the above steps have to be done by coding only and not manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "24f3c8c7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11812/3154054987.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnxt_button\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_attribute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'href'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#getting the link from the list for next page\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m     \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11812/3154054987.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnxt_button\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_attribute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'href'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#getting the link from the list for next page\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnxt_button\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_attribute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'href'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m         \u001b[1;31m#scrapping all product Url\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[0mproduct_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "driver=webdriver.Chrome(\"chromedriver.exe\") \n",
    "time.sleep(4)\n",
    "\n",
    "url = \" https://www.flipkart.com\"\n",
    "driver.get(url)\n",
    "time.sleep(6)\n",
    "\n",
    "\n",
    "\n",
    "search_product = driver.find_element(By.CLASS_NAME,'_3704LK')\n",
    "search_product.send_keys(\"sunglasses\")\n",
    "search_btn = driver.find_element(By.XPATH,'/html/body/div[1]/div/div[1]/div[1]/div[2]/div[2]/form/div/button')\n",
    "search_btn.submit()\n",
    "glass_brand=[]\n",
    "glass_description=[]\n",
    "glass_price=[]\n",
    "glass_discount=[]\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "start=0\n",
    "end=3\n",
    "for page in range(start,end):#for loop for scrapping 4 page\n",
    "    brands=driver.find_elements(By.CLASS_NAME,'_2WkVRV')#scraping brands name by class name='_2WkVRV'\n",
    "    for i in brands:\n",
    "        glass_brand.append(i.text)#appending the text in Brand list\n",
    "    desc=driver.find_elements(By.XPATH,'//a[@class=\"IRpwTa\"]')#scraping description from the xpath\n",
    "    for i in desc:\n",
    "        glass_description.append(i.text)#appending the description in list\n",
    "    prices=driver.find_elements(By.XPATH,\"//div[@class='_30jeq3']\")# scraping the price from the xpath\n",
    "    for i in prices:\n",
    "        glass_price.append(i.text)\n",
    "    nxt_button=driver.find_elements(By.XPATH,\"//a[@class='_1LKTO3']\")#scraping the list of buttons from the page\n",
    "    try:\n",
    "        driver.get(nxt_button[1].get_attribute('href'))#getting the link from the list for next page\n",
    "    except:\n",
    "        driver.get(nxt_button[10].get_attribute('href'))\n",
    "        #scrapping all product Url\n",
    "product_url = []\n",
    "start=0\n",
    "end=3\n",
    "for page in range(start,end):#for loop for scrapping 4 page\n",
    "    url=driver.find_elements(By.XPATH,\"//a[@class='_2UzuFa']\")\n",
    "    for i in url:\n",
    "        product_url.append(i.get_attribute('href'))\n",
    "    nxt_button=driver.find_elements(By.XPATH,\"//a[@class='_1LKTO3']\")\n",
    "    try:\n",
    "        driver.get(nxt_button[1].get_attribute('href'))     #getting the link from the list for next page\n",
    "    except:\n",
    "        driver.get(nxt_button[0].get_attribute('href'))\n",
    "# scraping discount from each product url\n",
    "for product in product_url:\n",
    "    driver.get(product)\n",
    "    try:\n",
    "        disc=driver.find_element(By.XPATH,\"/html/body/div[1]/div/div[3]/div[1]/div[2]/div[3]/div/div[3]/div[1]/div/div[3]/span\")# scraping the discount from the absolute xpath\n",
    "        glass_discount.append(disc.text)\n",
    "    except NoSuchElementException:\n",
    "        glass_discount.append(\"No Discount\")          \n",
    "len(glass_brand),len(glass_description),len(glass_price),len(glass_discount),len(product_url)\n",
    "#creating a dataframe\n",
    "df=pd.DataFrame({'Brand':glass_brand[:120],\n",
    "                'Description':glass_description[:120],\n",
    "                'Price':glass_price[:120],\n",
    "                'Discount':glass_discount[:120]})\n",
    "#printing dataframe\n",
    "df(head=100)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0d42a7b8",
   "metadata": {},
   "source": [
    "Q5: Scrape 100 reviews data from flipkart.com for iphone11 phone.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.flipkart.com/\n",
    "2. Enter ‚Äúiphone 11‚Äù in ‚ÄúSearch‚Äù field .\n",
    "3. Then click the search button.\n",
    "You will reach to the below shown webpage .\n",
    "As\n",
    "As shown in the above page you have to scrape the tick marked attributes.These are:\n",
    "1. Rating\n",
    "2. Review summary\n",
    "3. Full review\n",
    "4. You have to scrape this data for first 100 reviews.\n",
    "Note: All the steps required during scraping should be done through code only and not manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "64c304b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Short Review</th>\n",
       "      <th>Full Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>WORTH THE WAIT...and...GREAT SERVICE FROM FLIP...</td>\n",
       "      <td>I had deliberately delayed writing a review fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>iphone 6 plus</td>\n",
       "      <td>well was dreaming about iphone with big screen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>awesome\\n\\nscreen quality better than any othe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Must buy!</td>\n",
       "      <td>Its doenst need any review i guess so</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Classic !!!!</td>\n",
       "      <td>1.You never gonna feel the quality of this mob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Must buy!</td>\n",
       "      <td>Still using it in 2019 üòÇ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Best phone still @25-27K</td>\n",
       "      <td>Purchased at BBD @2016 Oct. After 5months of u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Terrific purchase</td>\n",
       "      <td>Using since 2018 and it still works well!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Good phone but not for the power user</td>\n",
       "      <td>Apple's iPhone series have been known for thei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Super!</td>\n",
       "      <td>This review is after 6 year of purchasing this...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Ratings                                       Short Review  \\\n",
       "0      NaN  WORTH THE WAIT...and...GREAT SERVICE FROM FLIP...   \n",
       "1      NaN                                      iphone 6 plus   \n",
       "2      NaN                                          Excellent   \n",
       "3      NaN                                          Must buy!   \n",
       "4      NaN                                       Classic !!!!   \n",
       "5      NaN                                          Must buy!   \n",
       "6      NaN                           Best phone still @25-27K   \n",
       "7      NaN                                  Terrific purchase   \n",
       "8      NaN              Good phone but not for the power user   \n",
       "9      NaN                                             Super!   \n",
       "\n",
       "                                         Full Review  \n",
       "0  I had deliberately delayed writing a review fo...  \n",
       "1  well was dreaming about iphone with big screen...  \n",
       "2  awesome\\n\\nscreen quality better than any othe...  \n",
       "3              Its doenst need any review i guess so  \n",
       "4  1.You never gonna feel the quality of this mob...  \n",
       "5                           Still using it in 2019 üòÇ  \n",
       "6  Purchased at BBD @2016 Oct. After 5months of u...  \n",
       "7          Using since 2018 and it still works well!  \n",
       "8  Apple's iPhone series have been known for thei...  \n",
       "9  This review is after 6 year of purchasing this...  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver=webdriver.Chrome(\"chromedriver.exe\") \n",
    "time.sleep(4)\n",
    "\n",
    "url = \" https://www.flipkart.com\"\n",
    "driver.get(url)\n",
    "time.sleep(6)\n",
    "search_product = driver.find_element(By.CLASS_NAME,'_3704LK')\n",
    "search_product.send_keys(\"iphone11\")\n",
    "search_btn = driver.find_element(By.XPATH,'/html/body/div[1]/div/div[1]/div[1]/div[2]/div[2]/form/div/button')\n",
    "search_btn.submit()\n",
    "\n",
    "# Creating empty list\n",
    "urls=[]\n",
    "short_review=[]\n",
    "complete_review=[]\n",
    "stars=[]\n",
    "time.sleep(2)\n",
    "\n",
    "#scraping 10 pages url\n",
    "url_1 = driver.find_elements(By.XPATH,\"//a[@class='ge-49M _2Kfbh8']\")\n",
    "for i in url_1:\n",
    "    urls.append(i.get_attribute('href'))\n",
    "    \n",
    "url_2 = driver.find_elements(By.XPATH,\"//a[@class='ge-49M']\")\n",
    "for i in url_2:\n",
    "    urls.append(i.get_attribute('href'))\n",
    "    \n",
    "urls\n",
    "time.sleep(4)\n",
    "\n",
    "\n",
    "for i in urls:\n",
    "    driver.get(i)\n",
    "    #for scrapping the number of stars\n",
    "for j in driver.find_elements(By.XPATH,\"//div[@class='col _2wzgFH K0kLPL']/div[1]/div[1]\"):\n",
    " stars.append(j.text)\n",
    "stars\n",
    "    #for scrapping the short review\n",
    "short_review=[]\n",
    "for k in driver.find_elements(By.XPATH,\"//p[@class='_2-N8zT']\"):\n",
    " short_review.append(k.text)\n",
    "\n",
    "short_review\n",
    "    #for scrapping the complete review\n",
    "for l in driver.find_elements(By.XPATH,\"//div[@class='t-ZTKy']/div/div\"):\n",
    " complete_review.append(l.text)\n",
    "complete_review\n",
    "        #Combining all the lists into a single dataframe\n",
    "#df=pd.DataFrame({'Ratings': stars,'Short Review': short_review,'Full Review': complete_review})\n",
    "\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "df['Ratings']= stars\n",
    "df['Short Review']= short_review\n",
    "df['Full Review']=complete_review\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6b19d00b",
   "metadata": {},
   "source": [
    "Q6: Scrape data for first 100 sneakers you find when you visit flipkart.com and search for ‚Äúsneakers‚Äù in the\n",
    "search field.\n",
    "You have to scrape 4 attributes of each sneaker:\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "As shown in the below image, you have to scrape the tick marked attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb49b6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "time.sleep(3)\n",
    "\n",
    "url = \" https://www.flipkart.com\"\n",
    "driver.get(url)\n",
    "#close log_in window\n",
    "#log_in_pop_up = driver.find_element_by_xpath(\"//button[@class='_2KpZ6l _2doB4z']\")\n",
    "#log_in_pop_up.click()\n",
    "\n",
    "\n",
    "search_product = driver.find_element_by_class_name('_3704LK')\n",
    "search_product.send_keys(\"sneakers\")\n",
    "search_btn = driver.find_element_by_xpath(\"//button[@class='L0Z3Pu']\")\n",
    "search_btn.click()\n",
    "#creating the empty list\n",
    "brand=[]\n",
    "description=[]\n",
    "price=[]\n",
    "discount=[]\n",
    "\n",
    "\n",
    "time.sleep(3)\n",
    "#scrapping the required details\n",
    "start=0\n",
    "end=4\n",
    "for page in range(start,end):#for loop for scrapping 4 page\n",
    "    brands=driver.find_elements_by_class_name('_2WkVRV')#scraping brands name by class name='_2WkVRV'\n",
    "    for i in brands:\n",
    "        brand.append(i.text)#appending the text in Brand list\n",
    "    prices=driver.find_elements_by_xpath(\"//div[@class='_30jeq3']\")# scraping the price from the xpath\n",
    "    for i in prices:\n",
    "        price.append(i.text)\n",
    "    disc=driver.find_elements_by_xpath(\"//div[@class='_3Ay6Sb']//span\")# scraping the discount from the xpath\n",
    "    for i in disc:\n",
    "        discount.append(i.text)\n",
    "    nxt_button=driver.find_elements_by_xpath(\"//a[@class='_1LKTO3']\")#scraping the list of buttons from the page\n",
    "    try:\n",
    "        driver.get(nxt_button[1].get_attribute('href'))#getting the link from the list for next page\n",
    "    except:\n",
    "        driver.get(nxt_button[0].get_attribute('href'))\n",
    "        \n",
    "        \n",
    "        \n",
    "time.sleep(2)        \n",
    "# Since in some records not getting description so try from inside of urls\n",
    "urls = []\n",
    "\n",
    "for page in range(0,4):#for loop for scrapping 4 page\n",
    "    \n",
    "    product_url = driver.find_elements_by_xpath(\"//a[@class='_2UzuFa']\")\n",
    "    for i in product_url:\n",
    "        urls.append(i.get_attribute('href'))\n",
    "         nxt_button=driver.find_elements_by_xpath(\"//a[@class='_1LKTO3']\")#scraping the list of buttons from the page\n",
    "    try:\n",
    "        driver.get(nxt_button[1].get_attribute('href'))#getting the link from the list for next page\n",
    "    except:\n",
    "        driver.get(nxt_button[0].get_attribute('href'))\n",
    "        \n",
    "time.sleep(2)        \n",
    "for i in urls:\n",
    "    driver.get(i)\n",
    "    desc=driver.find_elements_by_xpath('//span[@class=\"B_NuCI\"]')#scraping description from the xpath\n",
    "    for i in desc:\n",
    "        description.append(i.text)#appending the description in list\n",
    "time.sleep(2)        \n",
    "time.sleep(3)        \n",
    "#creating a dataframe\n",
    "df=pd.DataFrame({'Brand':brand[:100],\n",
    "                'Description':description[:100],\n",
    "                'Price':price[:100],\n",
    "                'Discount':discount[:100]})\n",
    "#printing dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d188043a",
   "metadata": {},
   "source": [
    "Q8: Go to webpage https://www.amazon.in/\n",
    "Enter ‚ÄúLaptop‚Äù in the search field and then click the search icon.\n",
    "Then set CPU Type filter to ‚ÄúIntel Core i7‚Äù as shown in the below image:\n",
    "AfterAfter setting the filters scrape first 10 laptops data. You have to scrape 3 attributesfor each laptop:\n",
    "1. Title\n",
    "2. Ratings\n",
    "3. Price\n",
    "ASSIGNMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3431bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \" https://www.amazon.in\"\n",
    "driver.get(url)\n",
    "search_laptop = driver.find_element_by_xpath('//*[@id=\"twotabsearchtextbox\"]')\n",
    "search_laptop.send_keys(\"laptops\")\n",
    "search_butn = driver.find_element_by_xpath(\"//*[@id='nav-search-submit-button']\")\n",
    "search_butn.click()\n",
    "search_process = driver.find_elements_by_xpath(\"//a[@class='a-link-normal s-navigation-item']/span\")\n",
    "for i in search_process:\n",
    "    if i.text==\"Intel Core i7\":\n",
    "        i.click()\n",
    "        break\n",
    "                                               \n",
    "search_process1 = driver.find_elements_by_xpath(\"//a[@class='a-link-normal s-navigation-item']/span\")\n",
    "for i in search_process1:\n",
    "    if i.text==\"Intel Core i9\":\n",
    "        i.click()\n",
    "        break\n",
    "title_tags3 = driver.find_elements_by_xpath(\"//h2[@class='a-size-mini a-spacing-none a-color-base s-line-clamp-2']\")\n",
    "title_tags3\n",
    "product_titles=[]\n",
    "for i in title_tags3[:18]:\n",
    "    product_titles.append(i.text)\n",
    "print(\"list of product titles:\",*product_titles,sep=('\\n\\n'))     \n",
    "product_stars = driver.find_elements_by_xpath(\"//div[@class='a-row a-size-small']\")\n",
    "product_stars\n",
    "product_sta=[]\n",
    "for i in product_stars[:18]:\n",
    "    product_sta.append(i.text)\n",
    "print(\"list of product stars:\",*product_sta,sep=('\\n\\n'))     \n",
    "\n",
    "product_money = driver.find_elements_by_xpath(\"//span[@class='a-price']\")\n",
    "product_money\n",
    "product_mon=[]\n",
    "for i in product_money[:18]:\n",
    "    product_mon.append(i.text)\n",
    "print(\"list of product money:\",*product_mon,sep=('\\n\\n'))     \n",
    "    \n",
    "    laptops = pd.DataFrame({})\n",
    "laptops['Titles'] =product_titles\n",
    "laptops['Money']  =product_mon\n",
    "laptops['Stars']  =product_sta\n",
    "laptops"
   ]
  },
  {
   "cell_type": "raw",
   "id": "df6698b4",
   "metadata": {},
   "source": [
    "Q9: Write a python program to scrape data for first 10 job results for Data Scientist Designation in Noida\n",
    "location. You have to scrape company name, No. of days ago when job was posted, Rating of the company.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.ambitionbox.com/\n",
    "2. Click on the Job option as shown in the image\n",
    "3.3. After reaching to the next webpage, In place of ‚ÄúSearch by Designations, Companies, Skills‚Äù enter\n",
    "‚ÄúData Scientist‚Äù and click on search button.\n",
    "ASSIGNMENT 2\n",
    "4. You will reach to the following web page click on location and in place of ‚ÄúSearch location‚Äù enter\n",
    "‚ÄúNoida‚Äù and select location ‚ÄúNoida‚Äù.\n",
    "5.5. Then scrape the data for the first 10 jobs results you get on the above shown page.\n",
    "6. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73520790",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
